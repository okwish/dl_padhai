{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "47AclhpxAPQD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sA2SiKs5AU2J",
    "outputId": "0fb9d0ca-ac70-492c-b1e3-67e63198f36f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzoE06BlBP3N"
   },
   "source": [
    "colab  \n",
    "we are running on a cloud device which has GPU available.  \n",
    "can add GPU:  \n",
    "edit - notebook setting - hardware accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqRTllvLBvqf",
    "outputId": "bc462563-6ff2-478f-d42a-e088723c5324"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DcfXvPKAByN-",
    "outputId": "a1243c57-b65a-41f8-ad7d-61681033924f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have access to a GPU \n",
    "\n",
    "torch.cuda.device(0) #gives a reference to device-object\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q1FZzYFkCErm"
   },
   "outputs": [],
   "source": [
    "# can reference the device: \n",
    "cuda0 = torch.device('cuda:0') \n",
    "#0 corresponds to location of the device. (there could be multiple devices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch variable  \n",
    "mentioning device  (default = cpu)  \n",
    "device is an attibute of variable object.\n",
    "\n",
    "a variable in gpu - operations will be in gpu itself.  \n",
    "(all variables in the operation in the same processor(gpu/cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F35dCzkzCpKN",
    "outputId": "a97d88a6-f004-4e00-dd02-26399d77f509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# specifying device.\n",
    "# if not specified, then its made in CPU \n",
    "# and operations on that will also be done in CPU\n",
    "\n",
    "a = torch.ones(3,2, device=cuda0) \n",
    "# create tensor on gpu and return the reference(can assign to a variable)\n",
    "# operations run on them will also run on the gpu.\n",
    "# notice the device in the object (in printed)\n",
    "print(a)\n",
    "\n",
    "b = torch.ones(3,2, device=cuda0) \n",
    "\n",
    "# 'device' refers to the GPU and 'host' referes to the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgMar-XtKIF5"
   },
   "source": [
    "### Comparing time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gks4mAuKL7c",
    "outputId": "83ee8950-95cd-49c1-faec-f3d2a534a3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 2 s, total: 40.9 s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2 = np.random.randn(10000,10000)\n",
    "    B2 = np.random.randn(10000,10000)\n",
    "    np.add(B2, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMpY0wkCKPL1",
    "outputId": "c02e4ab7-feb7-4881-d119-45c017128f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 1.64 s, total: 12.3 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2_cpu = torch.randn([10000, 10000])\n",
    "    B2_cpu = torch.randn([10000, 10000])\n",
    "    B2_cpu.add_(A2_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU, torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1-HuJuJKQY0",
    "outputId": "a09127e3-6a21-48ba-86a6-6f489ff5f1ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 ms, sys: 1.06 ms, total: 4.97 ms\n",
      "Wall time: 9.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2_gpu = torch.randn([10000, 10000], device=cuda0)\n",
    "    B2_gpu = torch.randn([10000, 10000], device=cuda0)\n",
    "    B2_gpu.add_(A2_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMIfo7ujLUUT"
   },
   "source": [
    "1000 times improvement going to GPU. very significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "BN2MirROKRkY",
    "outputId": "c8370871-8b76-4e19-8b6d-fba200218648"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2 = np.random.randn(10000,10000)\n",
    "    B2 = np.random.randn(10000,10000)\n",
    "    np.matmul(B2, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "vMpeggeiKUtk",
    "outputId": "85b73b52-0618-4c72-e620-a326fb3ebbe2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2_cpu = torch.randn([10000, 10000])\n",
    "    B2_cpu = torch.randn([10000, 10000])\n",
    "    torch.matmul(A2_cpu, B2_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyE49MA1KV6I",
    "outputId": "72a05ca6-3a24-40e6-c8eb-fd486c6009b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 507 ms, sys: 161 ms, total: 668 ms\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    A2_gpu = torch.randn([10000, 10000], device=cuda0)\n",
    "    B2_gpu = torch.randn([10000, 10000], device=cuda0)\n",
    "    torch.matmul(A2_gpu, B2_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMeYQdi4Lte-"
   },
   "source": [
    "speed up operations like multication are also good as they are parallelisable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQp_-HVqHZYj"
   },
   "source": [
    "### speed comparison for autodiff,..  :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch variable(tensor) : \n",
    "\n",
    "**requires_grad = True** => all operations with the variable is update in the computation table  \n",
    "also update the corresponding derivative(gradient) formula for backpropagation step.  \n",
    "\n",
    "derivative of something - **wrt** those variables.  \n",
    "\n",
    "one_element_tensor.item()  - numerical value  \n",
    "\n",
    "obj_var.backward()  \n",
    "obj_var-> objective function  \n",
    "do backprop(ie, computes gradient) - and store in **each torch_var.grad** (where, torch_var is torch variable, ie with requires_grad = True.)\n",
    "\n",
    "\n",
    "\"with torch.no_grad():\"    \n",
    "what in the block is not updated in the computation table.  \n",
    "computations to be done - but not in the forward pass, etc..  \n",
    "eg: gradient update step  \n",
    "\n",
    "var.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true system : y= (3x1 + 3x2 + ....) -2 \n",
    "\n",
    "\n",
    "# model : y_hat = W.X + b\n",
    "# (vector input, scalar output model. parameters:W,b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch, CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57zjXYpRHfbH",
    "outputId": "3599029e-7540-40b5-b740-1373cfad7f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "CPU times: user 23.8 s, sys: 350 ms, total: 24.1 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 0.001\n",
    "N = 1000000 #large number of parameters\n",
    "epochs = 2000\n",
    "\n",
    "W = torch.rand([N], requires_grad=True)\n",
    "b = torch.ones([1], requires_grad=True)\n",
    "\n",
    "# print(torch.mean(W).item(), b.item())\n",
    "\n",
    "for i in range(epochs):\n",
    "  \n",
    "    X = torch.randn([N])\n",
    "    y = torch.dot( 3*torch.ones([N]), X ) - 2 \n",
    "  \n",
    "    y_hat = torch.dot(W, X) + b\n",
    "    loss = torch.sum((y_hat - y)**2)\n",
    "    # ??? sum\n",
    "  \n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print(torch.mean(W).item(), b.item()) \n",
    "\n",
    "# all weights be learned to 3, as that is the true reln. \n",
    "# bias be learned to 2 (true value)\n",
    "\n",
    "# here since all w values are same in actual reln.. \n",
    "# printing mean of all w, instead of printing all w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch, GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 0.001\n",
    "N = 1000000 #large number of parameters\n",
    "epochs = 200\n",
    "\n",
    "W_gpu = torch.rand([N], requires_grad=True, device=cuda0)\n",
    "b_gpu = torch.ones([1], requires_grad=True, device=cuda0)\n",
    "\n",
    "# print(torch.mean(W).item(), b.item())\n",
    "\n",
    "for i in range(epochs):\n",
    "  \n",
    "    X_gpu = torch.randn([N], device=cuda0)\n",
    "    # operation on gpu-variable. return will also be a gpu-variable\n",
    "    y_gpu = torch.dot( 3*torch.ones([N], device=cuda0), X ) - 2 # returns gpu variable\n",
    "  \n",
    "    y_hat_gpu = torch.dot(W_gpu, X_gpu) + b_gpu\n",
    "    loss = torch.sum((y_hat_gpu - y_gpu)**2)\n",
    "  \n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        W_gpu -= learning_rate * W_gpu.grad\n",
    "        b_gpu -= learning_rate * b_gpu.grad\n",
    "    \n",
    "        W_gpu.grad.zero_()\n",
    "        b_gpu.grad.zero_()\n",
    "\n",
    "print(torch.mean(W_gpu).item(), b_gpu.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdENaFKBH4Vh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
