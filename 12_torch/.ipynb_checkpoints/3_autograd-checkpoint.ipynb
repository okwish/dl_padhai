{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a6b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07fde917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "dev =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97aac5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc418308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75310f9b",
   "metadata": {},
   "source": [
    "#### Torch \"variable\"\n",
    "\n",
    "\n",
    "##### upto version 0.3.1\n",
    "from torch.autograd import Variable  \n",
    "Variable(tr)  \n",
    "in older version any tensor involved in a computation that needed to be tracked by autograd had to be wrapped in a Variable  \n",
    "\n",
    "##### newer than 0.3.1\n",
    "does not actually require the use of \"Variable\"  \n",
    "\n",
    "In 0.4 the functionality of Variable was merged into the Tensor class. In modern PyTorch, you simply have to set the requires_grad attribute of the tensor to achieve the same behavior.  \n",
    "\n",
    "me - \"calling\" torch **variable** -> that which is tracked in computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec9db",
   "metadata": {},
   "source": [
    "## Autograd  \n",
    "\n",
    "automatic computation of gradients  \n",
    "\n",
    "specify 'requires_grad' parameter in tensor definition.  \n",
    "\n",
    "telling that :-  can make functions using those variable; and differentiate (the functions) **w.r.t** those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617aca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# specify 'requires_grad' parameter in tensor definition.\n",
    "\n",
    "X2 = torch.ones([3,2], requires_grad=True)\n",
    "print(X2)\n",
    "\n",
    "# notice requires_grad in printed \n",
    "\n",
    "# OR\n",
    "\n",
    "# var.requires_grad_()\n",
    "# set requires grad afterwards (in place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f349d",
   "metadata": {},
   "source": [
    "when a new variable is made using a variable.. (some relation.. ), because we set requires_grad to True, it knows we want derivative.. so the analytical thing of that term in the chain, etc.. will be logged??\n",
    "\n",
    "build **computation graph** , everytime we make a new variable with it.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae99542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.],\n",
      "        [6., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[37., 37.],\n",
      "        [37., 37.],\n",
      "        [37., 37.]], grad_fn=<AddBackward0>)\n",
      "tensor(222., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Y2 = X2+5\n",
    "Z2 = Y2*Y2 +1\n",
    "t = torch.sum(Z2) #add up all values. single number\n",
    "\n",
    "print(Y2)\n",
    "print(Z2)\n",
    "print(t)\n",
    "\n",
    "# the book keeping of the relations is done.\n",
    "\n",
    "# notice 'addbackward' in two of them, and 'sumbackward' in third. \n",
    "# book keeping of the relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a2b6f",
   "metadata": {},
   "source": [
    "**fn.backward()**    \n",
    "find partial derivative of function **wrt** all with req_grad=True  \n",
    "store those in `var.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31dfe5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12.],\n",
       "        [12., 12.],\n",
       "        [12., 12.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above(doing operations) is like a forward pass.. \n",
    "\n",
    "t.backward() # do derivation computation and store to 'grad' parameter\n",
    "# backward starting from t\n",
    "# now derivative can be accessed with 'grad'\n",
    "\n",
    "X2.grad # derivative of t wrt X2. \n",
    "# DERIVATIVE AFTER SUBSTITUTING THE CURRENT VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6fea32",
   "metadata": {},
   "source": [
    "objective_fn_var.backward()   \n",
    "do backprop(ie, computes gradient) - and store in **each torch_var.grad attribute** (where, torch_var is torch variable, ie with requires_grad = True.)   \n",
    "\n",
    "(derivative **wrt** that variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "278bfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any cascading set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc91f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.0025, 12.0025],\n",
       "        [12.0025, 12.0025],\n",
       "        [12.0025, 12.0025]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2 = 1/(1+torch.exp(-Y2)) #sigmoid\n",
    "# note : \"torch.exp\" - torch function.\n",
    "s = torch.sum(R2)\n",
    "\n",
    "s.backward()\n",
    "X2.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b00d6",
   "metadata": {},
   "source": [
    "above - called backward on single valued variable.  \n",
    "\n",
    "### backward on a tensor-variable(not scalar):  \n",
    "\n",
    "additional argument passed with backward  \n",
    "find partial derivative and also does an element wise multiplication with the argument passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "007a11c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36.0148, 36.0148],\n",
       "        [36.0148, 36.0148],\n",
       "        [36.0148, 36.0148]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2 = 1/(1+torch.exp(-Y2))\n",
    "\n",
    "A2 = torch.ones(R2.shape)\n",
    "\n",
    "R2.backward(A2) #should be called with an arg = tensor with same dimension  \n",
    "\n",
    "# finding derivative of 'R2' wrt X2; and also does an element wise multiplication with 'A2'.\n",
    "\n",
    "X2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d3c47",
   "metadata": {},
   "source": [
    "this is made this way - so that we can cascade our chain rule through multiple function.\n",
    "\n",
    "x->r->s (say)\n",
    "\n",
    "s.backward(r.backward)\n",
    "\n",
    "ds/dr . dr/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79f879",
   "metadata": {},
   "source": [
    "The .grad attribute of a Tensor that is not a \"leaf\" Tensor is being accessed. Its .grad attribute **won't** be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead.  \n",
    "\n",
    "leaf Tensor - those which we make first with require_grad=True. Want derivative wrt those.  \n",
    "non-leaf Tensor - operation returns of leaf-tensors; but backward() not called on it.\n",
    "\n",
    "\n",
    "\n",
    "can find wrt defined variable. (ie with req_grad thing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8afcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a2c4cf9",
   "metadata": {},
   "source": [
    "### Autodiff eg that looks like what we have been doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cce48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(222.0467, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "X = torch.randn([20, 1], requires_grad=True)\n",
    "Y = 3*X - 2\n",
    "\n",
    "# parameters\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "# model\n",
    "Y_hat = w*X + b\n",
    "\n",
    "# loss\n",
    "loss = torch.sum((Y_hat - Y)**2)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41799d40",
   "metadata": {},
   "source": [
    "all variables involved be with req_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ecd2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-37.9472]) tensor([122.7330])\n"
     ]
    }
   ],
   "source": [
    "# find gradient\n",
    "loss.backward()\n",
    "\n",
    "# found and stored in var.grad\n",
    "print(w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13044d26",
   "metadata": {},
   "source": [
    "### Train w, b in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee9e2e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "1.5878593921661377 0.10683983564376831\n",
      "2.037818670272827 -0.6759647130966187\n",
      "2.7846431732177734 -1.275583028793335\n",
      "2.810288429260254 -1.54954993724823\n",
      "2.8761017322540283 -1.7338801622390747\n",
      "2.920194625854492 -1.8425291776657104\n",
      "2.9628212451934814 -1.9117941856384277\n",
      "2.9826712608337402 -1.9483814239501953\n",
      "2.9959774017333984 -1.9701703786849976\n",
      "2.997032642364502 -1.9820444583892822\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "print(w.item(), b.item())\n",
    "\n",
    "for i in range(10):\n",
    "    X = torch.randn([20, 1])\n",
    "    # different x for each iteration - to simulate something like batch inputs\n",
    "    Y = 3 * X - 2\n",
    "\n",
    "    # forward pass\n",
    "    Y_hat = w * X + b\n",
    "    \n",
    "    # loss\n",
    "    loss = torch.sum((Y_hat - Y) ** 2)\n",
    "\n",
    "    # backward (finding gradient using forward-pass-values)\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    with torch.no_grad():\n",
    "        # update parameters\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "        # set gradients to 0\n",
    "        w.grad.zero_() # make zero in place. (grad attibute of w-object)\n",
    "        b.grad.zero_()\n",
    "\n",
    "    print(w.item(), b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383b67b",
   "metadata": {},
   "source": [
    "**forward pass is only to get the values - which are needed in gradient calculation**  \n",
    "\n",
    "- forward with the current parameters (updates intermediate values)\n",
    "- compute gradients (uses the intermediate values)\n",
    "- update parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfca21",
   "metadata": {},
   "source": [
    "the update line is like making another relation.  \n",
    "so if we write that w/o 'torch.no_grad()' , then pytorch will think its another reln and will continue to build the computation graph.\n",
    "\n",
    "**with torch.no_grad():** -> says don't do the book keeping.  \n",
    "what in the block is not updated in the computation table.  \n",
    "computations to be done - but not in the forward pass, etc..   \n",
    "\n",
    "\n",
    "var.grad.zero_() -> make var.grad=0 (inplace)  \n",
    "\n",
    "why explicitly set to 0? won't it overwrite?  \n",
    "by default - find gradient and adds to the current??   \n",
    "(so that something like finding gradient for each data point - adding up can be implemented)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014d1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d37e8d",
   "metadata": {
    "id": "rQp_-HVqHZYj"
   },
   "source": [
    "### speed comparison for autodiff,..  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2db6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true system : y= (3x1 + 3x2 + ....) -2 \n",
    "\n",
    "\n",
    "# model : y_hat = W.X + b\n",
    "# (vector input, scalar output model. parameters:W,b )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ce998",
   "metadata": {},
   "source": [
    "#### torch, CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ffee428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57zjXYpRHfbH",
    "outputId": "3599029e-7540-40b5-b740-1373cfad7f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "CPU times: user 1min 41s, sys: 955 ms, total: 1min 42s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 0.001\n",
    "N = 1000000 #large number of parameters\n",
    "epochs = 2000\n",
    "\n",
    "W = torch.rand([N], requires_grad=True)\n",
    "b = torch.ones([1], requires_grad=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "      \n",
    "    # data\n",
    "    X = torch.randn([N])\n",
    "    y = torch.dot( 3*torch.ones([N]), X ) - 2 \n",
    "  \n",
    "    # forward pass (prediction)\n",
    "    y_hat = torch.dot(W, X) + b\n",
    "    \n",
    "    # loss\n",
    "    loss = torch.sum((y_hat - y)**2)\n",
    "      \n",
    "    # compute gradient\n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # update parameters\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print(torch.mean(W).item(), b.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31607339",
   "metadata": {},
   "source": [
    "#### torch, GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebac330",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "CPU times: user 304 ms, sys: 100 ms, total: 405 ms\n",
      "Wall time: 403 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "learning_rate = 0.001\n",
    "N = 1000000 #large number of parameters\n",
    "epochs = 200\n",
    "\n",
    "# parameters in GPU\n",
    "W_gpu = torch.rand([N], requires_grad=True, device=dev)\n",
    "b_gpu = torch.ones([1], requires_grad=True, device=dev)\n",
    "\n",
    "for i in range(epochs):\n",
    "      \n",
    "    # data in GPU    \n",
    "    X_gpu = torch.randn([N], device=dev)\n",
    "    # operation on gpu-variable. return will also be a gpu-variable\n",
    "    y_gpu = torch.dot( 3*torch.ones([N], device=dev), X_gpu ) - 2 # returns gpu variable\n",
    "  \n",
    "    # forward pass\n",
    "    y_hat_gpu = torch.dot(W_gpu, X_gpu) + b_gpu\n",
    "    \n",
    "    # loss\n",
    "    loss = torch.sum((y_hat_gpu - y_gpu)**2)\n",
    "  \n",
    "    # compute gradient\n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        # update parameters\n",
    "        W_gpu -= learning_rate * W_gpu.grad\n",
    "        b_gpu -= learning_rate * b_gpu.grad\n",
    "    \n",
    "        W_gpu.grad.zero_()\n",
    "        b_gpu.grad.zero_()\n",
    "\n",
    "print(torch.mean(W_gpu).item(), b_gpu.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff56cb",
   "metadata": {
    "id": "tdENaFKBH4Vh"
   },
   "outputs": [],
   "source": [
    "# all weights be learned to 3, as that is the true reln. \n",
    "# bias be learned to 2 (true value)\n",
    "\n",
    "# here since all w values are same in actual reln.. \n",
    "# printing mean of all w, instead of printing all w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae190caa",
   "metadata": {},
   "source": [
    "both **data and parameters** in GPU  \n",
    "\n",
    "operation on gpu-variable. return will also be a gpu-variable.  \n",
    "\n",
    "**all should be in same place**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bf069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
