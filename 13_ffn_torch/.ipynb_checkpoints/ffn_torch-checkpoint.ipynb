{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FF network using pytorch tensors, autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN abstractions available in pytorch\n",
    "\n",
    "pytorch.nn which has functional, linear, sequential, optimizer\n",
    "\n",
    "make code more compact, efficient, easier to read, maintain.\n",
    "\n",
    "efficient, clean way to write code.  \n",
    "make benifit of all the libraries.(to work with DL at scale.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajivlrTGqhXD",
    "outputId": "560ef51d-5085-4ed3-88d2-dd3dc83040bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa148399b30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0) #torch seed for random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoOS7L2AqsZG",
    "outputId": "1d19d1f2-2d33-4de1-bc27-d83c0e3f10f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 2) (250, 2) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\n",
    "\n",
    "XX_train, XX_val, Y_train, Y_val = train_test_split(data, labels, \n",
    "                                                  stratify=labels, random_state=0)\n",
    "print(XX_train.shape, XX_val.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data as torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yF1DDagnt41G",
    "outputId": "1d1af76d-acbe-4f18-aeb2-7cb81566441a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 2]) torch.Size([750])\n"
     ]
    }
   ],
   "source": [
    "XX_train, Y_train, XX_val, Y_val = map(torch.tensor, (XX_train, Y_train, XX_val, Y_val)) \n",
    "\n",
    "# map the function to all and return all. - instead of calling on each one by one.\n",
    "\n",
    "print(XX_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai7jqn63tPgw"
   },
   "source": [
    "**Using torch tensor, autograd manaully.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN3AEQFbuMvs"
   },
   "outputs": [],
   "source": [
    "# forward prop given input. return ouput.\n",
    "\n",
    "# XX - a single datapoint(tensor) \n",
    "# or all datapoints(tensor)-that case returned o/p will be a set of outputs for each.(tensor)\n",
    "\n",
    "\n",
    "def model(XX):\n",
    "    AA1 = torch.matmul(XX, Wmat1) + Bvec1  # (N, 2) x (2, 2) -> (N, 2)\n",
    "    HH1 = AA1.sigmoid()  # (N, 2)\n",
    "    # sigmoid - torch fn that can be called on a tensor.\n",
    "\n",
    "    AA2 = torch.matmul(HH1, Wmat2) + Bvec2  # (N, 2) x (2, 4) -> (N, 4)\n",
    "\n",
    "    # softmax layer => softmax on preactivation values\n",
    "\n",
    "    # softmax\n",
    "    HH2 = AA2.exp() / AA2.exp().sum(-1).unsqueeze(-1)  # (N, 4)\n",
    "    # chaining way.\n",
    "    # exp() - exponential on each element.\n",
    "    # sum(which axis)  '-1' -> sum along last dimension.\n",
    "\n",
    "    # what sum returns will be one dimension lesser than the input. \n",
    "    # eg: if input is 3D tensor, sum gives 2D tensor\n",
    "    # this causes a dimension mismatch in the division opearaion.\n",
    "\n",
    "    # therefore we add back the dimension we lost while using sum \n",
    "    # - using the unsqueeze function.\n",
    "\n",
    "    # add dimension to -1 position - as that is where we took sum.\n",
    "\n",
    "    # unsqueeze -> add a dimension in the specified axis\n",
    "    # -1 => add dimension in last\n",
    "\n",
    "    return HH2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = -1  \n",
    "\n",
    "unsqueeze(axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAnyJgU-zL8t",
    "outputId": "5f607b82-5157-4df2-9953-9384525818bf"
   },
   "outputs": [],
   "source": [
    "#sum\n",
    "A3 = torch.ones(5,2,3)\n",
    "print(A3.shape)\n",
    "print(A3.sum(0).shape)\n",
    "print(A3.sum(1).shape)\n",
    "print(A3.sum(-1).shape)\n",
    "\n",
    "\n",
    "#unsqueeze\n",
    "\n",
    "A3 = torch.ones(5,2,3)\n",
    "print(A3.shape)\n",
    "print(A3.unsqueeze(1).shape)\n",
    "print(A3.unsqueeze(-1).shape)\n",
    "\n",
    "# add a 1 in the dimension - in the specified position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWVBnzSU1rn4"
   },
   "source": [
    "chaining functions\n",
    "\n",
    "calling one returns a tensor, on which another one can be called, and so on.. \n",
    "\n",
    "this makes code easier to follow and maintain(can understand what is going on easily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4RahXcN4LX2"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "# cross entropy loss\n",
    "# -log(prob corresponding to the true one)\n",
    "# sum of such for all datapoints (sum or mean)\n",
    "\n",
    "# Y_hat = probability distribution\n",
    "# y = true 'label' (NOT one-hot)\n",
    "\n",
    "def loss_fn(YY_hat, yy):\n",
    "    return -( YY_hat[ range(yy.shape[0]), yy ].log() ).mean()\n",
    "\n",
    "\n",
    "# element at 'y' position from each Y_hat\n",
    "# y_hat - N x classes\n",
    "# Y_hat[range(YY_hat.shape[0]), yy]  -> y-th in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor\\[ list_x, list_y ] - all element wise coordinate pairs  \n",
    "\n",
    "tensor[:, list] - all possible combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BB6PGLco70zS",
    "outputId": "9fdf0582-7d74-4a23-b872-24ba2fa63b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3000, 0.8000])\n",
      "tensor([[0.3000, 0.1000],\n",
      "        [0.0500, 0.8000]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.8, 0.1, 0.05, 0.05]])\n",
    "y = torch.tensor([2, 0])\n",
    "\n",
    "print( y_hat[range(y_hat.shape[0]), y] )\n",
    "print( y_hat[:, y] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-MguUMQ6Ov5"
   },
   "outputs": [],
   "source": [
    "def accuracy(YY_hat, yy):\n",
    "    pp = torch.argmax(YY_hat, dim=1) #prediction = max probability\n",
    "    return (pp == yy).float().mean()\n",
    "\n",
    "\n",
    "# .float() convert to float.\n",
    "# pred==y -> bool of correct preds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights(xavier)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "Wmat1 = torch.randn(2, 2) / math.sqrt(2)\n",
    "Wmat2.requires_grad_()  # enabling gradient after making tensor.\n",
    "# in place\n",
    "Bvec1 = torch.zeros(2, requires_grad=True)\n",
    "\n",
    "Wmat2 = torch.randn(2, 4) / math.sqrt(2)\n",
    "Wmat2.requires_grad_()\n",
    "Bvec2 = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "# require grad - set. as we want derviatives wrt the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here these parameters are like global variables. \n",
    "# They are accessed in the forward pass function , .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "rIq3c-MS6QjL",
    "outputId": "3438a030-6139-4854-c271-efe0123a0385"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "epochs = 10000\n",
    "\n",
    "XX_train = XX_train.float()\n",
    "Y_train = Y_train.long()  # as its an index.\n",
    "\n",
    "# book keeping\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "# each epoch:\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward prop - called on whole train dataset.\n",
    "    YY_hat = model(XX_train)\n",
    "    # YY_hat is series of output of each. N x o/p dim\n",
    "    \n",
    "    # loss 'variable':\n",
    "    loss = loss_fn(YY_hat, Y_train)  \n",
    "    # need this line as here is the loss-variable made\n",
    "    \n",
    "    # gradient on loss.\n",
    "    loss.backward()\n",
    "    \n",
    "    # loggin loss, accuracy - each epoch\n",
    "    loss_arr.append(loss.item())  # item gives the value.\n",
    "    acc_arr.append(accuracy(YY_hat, Y_train))\n",
    "\n",
    "    # updating parameters\n",
    "    with torch.no_grad():  # so that not treated as new variables\n",
    "        Wmat1 -= Wmat1.grad * learning_rate\n",
    "        Bvec1 -= Bvec1.grad * learning_rate\n",
    "        Wmat2 -= Wmat2.grad * learning_rate\n",
    "        Bvec2 -= Bvec2.grad * learning_rate\n",
    "\n",
    "        # make gradients 0.\n",
    "        Wmat1.grad.zero_()\n",
    "        Bvec1.grad.zero_()\n",
    "        Wmat2.grad.zero_()\n",
    "        Bvec2.grad.zero_()\n",
    "\n",
    "# plot the logged error, accuracy\n",
    "# vs epoch\n",
    "plt.plot(loss_arr, \"r-\")\n",
    "plt.plot(acc_arr, \"b-\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss before training: \", loss_arr[0])\n",
    "print(\"Loss after training: \", loss_arr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above - manually using tensor. vectorised code, plus BP using backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ6vH1VYLqXp"
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "from tqdm import tqdm_notebook \n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UkZaVQP6Rzk"
   },
   "source": [
    "Pytorch modules specifically for writing NNs.\n",
    "\n",
    "**NN, Optim** modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK3AtFSbD-2o"
   },
   "source": [
    "### nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBMVLjyYA_lY"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "-LV0Wk_5BatK",
    "outputId": "867619bb-f753-47e4-b902-ff73d1c493e4"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "weights1 = torch.randn(2, 2) / math.sqrt(2)\n",
    "weights1.requires_grad_()\n",
    "bias1 = torch.zeros(2, requires_grad=True)\n",
    "\n",
    "weights2 = torch.randn(2, 4) / math.sqrt(2)\n",
    "weights2.requires_grad_()\n",
    "bias2 = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.2\n",
    "epochs = 10000\n",
    "\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_hat = model(X_train)\n",
    "    # change is here\n",
    "    # torch function to compute cross entropy\n",
    "    # given turn output, model output(prob dist.)\n",
    "    loss = F.cross_entropy(y_hat, Y_train)\n",
    "    loss.backward()\n",
    "    loss_arr.append(loss.item())\n",
    "    acc_arr.append(accuracy(y_hat, Y_train))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weights1 -= weights1.grad * learning_rate\n",
    "        bias1 -= bias1.grad * learning_rate\n",
    "        weights2 -= weights2.grad * learning_rate\n",
    "        bias2 -= bias2.grad * learning_rate\n",
    "        weights1.grad.zero_()\n",
    "        bias1.grad.zero_()\n",
    "        weights2.grad.zero_()\n",
    "        bias2.grad.zero_()\n",
    "\n",
    "plt.plot(loss_arr, \"r-\")\n",
    "plt.plot(acc_arr, \"b-\")\n",
    "plt.show()\n",
    "print(\"Loss before training\", loss_arr[0])\n",
    "print(\"Loss after training\", loss_arr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ2VNZP2EF6D"
   },
   "source": [
    "### nn.parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL8uy48aEG4S"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEvm8tzgEjYh"
   },
   "source": [
    "inheriting from a parent class. \n",
    "\n",
    "parent class in paranthesis in class definition.\n",
    "\n",
    "and in init class super().init -> like calling constructor of parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpkvODCiEG1Y"
   },
   "outputs": [],
   "source": [
    "# class for model\n",
    "\n",
    "\n",
    "class FirstNetwork(nn.Module):  # inherit from nn.Module class(parent)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # making weights, biases as before - but wrapping in nn.Parameter()\n",
    "        # so model knows that they are parameters.\n",
    "        # don't need explicit req_grad\n",
    "        self.weights1 = nn.Parameter(torch.randn(2, 2) / math.sqrt(2))\n",
    "        self.bias1 = nn.Parameter(torch.zeros(2))\n",
    "        self.weights2 = nn.Parameter(torch.randn(2, 4) / math.sqrt(2))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(4))\n",
    "\n",
    "    def forward(self, X):  # same as before\n",
    "        a1 = torch.matmul(X, self.weights1) + self.bias1\n",
    "        h1 = a1.sigmoid()\n",
    "        a2 = torch.matmul(h1, self.weights2) + self.bias2\n",
    "        h2 = a2.exp() / a2.exp().sum(-1).unsqueeze(-1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZOtxi7uF2kY"
   },
   "source": [
    "inherited from nn.Module\n",
    "\n",
    "making a object - calling it like a function on something - calls the 'forward' function.\n",
    "\n",
    "thats how nn.Module is written. it also has a forward fn.. ? and we are overriding that in the subclass we make...??\n",
    "\n",
    "\n",
    "but object(..) -> what is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOuLTdXpEUVh"
   },
   "outputs": [],
   "source": [
    "# seperately wrinting fit function (not in class.)\n",
    "\n",
    "\n",
    "def fit(epochs=1000, learning_rate=1):\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "    # through each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # fn is object of above class(to be made.)\n",
    "        # calling fn like a fn - call forward function\n",
    "        # because it inherited from nn.Module.\n",
    "        y_hat = fn(X_train)\n",
    "\n",
    "        # torch fn to compute loss\n",
    "        loss = F.cross_entropy(y_hat, Y_train)\n",
    "\n",
    "        loss_arr.append(loss.item())\n",
    "        acc_arr.append(accuracy(y_hat, Y_train))\n",
    "\n",
    "        # gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # before - we manually wrote update lies for each parameters.\n",
    "        # now doing it in better way using nn.Parameters\n",
    "        with torch.no_grad():\n",
    "            for param in fn.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "            fn.zero_grad()  # set all gradients(of all params) to 0.\n",
    "\n",
    "        ## nn.Parameters - provide short hand for updating parameters\n",
    "        ## and reseting gradients to 0.\n",
    "\n",
    "    plt.plot(loss_arr, \"r-\")\n",
    "    plt.plot(acc_arr, \"b-\")\n",
    "    plt.show()\n",
    "    print(\"Loss before training\", loss_arr[0])\n",
    "    print(\"Loss after training\", loss_arr[-1])\n",
    "\n",
    "\n",
    "# now the fit function doesn't have to care about how many sets\n",
    "# of weights and biases are there in the model, because all of them\n",
    "# are wrapped in parameters - they can be iterated over in fit.\n",
    "# any number of sets is ok.\n",
    "\n",
    "\n",
    "# fit function can remain independent of the model.\n",
    "# not assuming any knowledge of the model above in fit()\n",
    "\n",
    "# same thing - for any model\n",
    "\n",
    "# this is design template in the framework.\n",
    "# keep fit function as a core function - and pass on to it the model, optimizer, hyper parameters, etc.\n",
    "\n",
    "# good programming style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWBaTk3rL_91"
   },
   "source": [
    "**fit function**\n",
    "\n",
    "fit function can remain independent of the model.  \n",
    "not assuming any knowledge of the model above in fit()  \n",
    "\n",
    "same thing - for any model\n",
    "\n",
    "this is design template in the framework.  \n",
    "keep fit function as a core function - and pass on to it the model, optimizer, hyper parameters, etc. \n",
    "\n",
    "good programming style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "OJpSjYl-JU6r",
    "outputId": "8a755d75-a9ea-483a-d3ee-3d95601f22f8"
   },
   "outputs": [],
   "source": [
    "fn = FirstNetwork()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBX7aa9uMRfl"
   },
   "source": [
    "### nn.linear\n",
    "\n",
    "instead of manually doing z = Wa + b\n",
    "\n",
    "all these are 'programming abstractions'  \n",
    "we are using common programming constucts throughout, so why not abstract them using such things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlgcn-zCMYy8"
   },
   "outputs": [],
   "source": [
    "class FirstNetwork_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        self.lin1 = nn.Linear(2, 2)\n",
    "        # automatically internally have weights,biases.\n",
    "        # above- 2x2 weights, 2x1 bias\n",
    "        # they will be wrapped in parameters.. also.\n",
    "        self.lin2 = nn.Linear(2, 4)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # instead of writing the matmul line - using linear.\n",
    "        a1 = self.lin1(X)\n",
    "        h1 = a1.sigmoid()\n",
    "        a2 = self.lin2(h1)\n",
    "        h2 = a2.exp() / a2.exp().sum(-1).unsqueeze(-1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "izrbNX8qNxOA",
    "outputId": "5a18b704-7ddc-4c9e-c175-53201a129efe"
   },
   "outputs": [],
   "source": [
    "fn = FirstNetwork_v1()\n",
    "fit()\n",
    "\n",
    "# notice not changing fit()\n",
    "# nn.linear - internally does same - parameters, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMRXv_wCOl-C"
   },
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOjllyt2Obe7"
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsnPf5UNO-Hx"
   },
   "outputs": [],
   "source": [
    "## fit function using optim.\n",
    "\n",
    "\n",
    "def fit_v1(epochs=1000, learning_rate=1):\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "\n",
    "    opt = optim.SGD(fn.parameters(), lr=learning_rate)\n",
    "    # stochastic gradient optimizer (?)\n",
    "    # can implement multiple things with this - specified as additional args - like momentum, nestrov, ..adam, ...\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = fn(X_train)\n",
    "        # loss\n",
    "        loss = F.cross_entropy(y_hat, Y_train)\n",
    "\n",
    "        loss_arr.append(loss.item())\n",
    "        acc_arr.append(accuracy(y_hat, Y_train))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # instead of iterating through parameters and updating manually\n",
    "        opt.step()  # BP step. updating all parameters.\n",
    "        opt.zero_grad()  # reseting gradients of all parameters\n",
    "\n",
    "    plt.plot(loss_arr, \"r-\")\n",
    "    plt.plot(acc_arr, \"b-\")\n",
    "    plt.show()\n",
    "    print(\"Loss before training\", loss_arr[0])\n",
    "    print(\"Loss after training\", loss_arr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "xHHXqXNVRp-n",
    "outputId": "625c4411-9d8d-4aea-bc1e-a05875d2430f"
   },
   "outputs": [],
   "source": [
    "fn = FirstNetwork_v1()\n",
    "fit_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q3mivb9S7zE"
   },
   "source": [
    "### nn.sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zdWXbwVTBYd"
   },
   "outputs": [],
   "source": [
    "class FirstNetwork_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # giving the series of operations that the data go through\n",
    "        # linear => linear combination\n",
    "        # call net (network)\n",
    "        # this in turn do the linear, .. which in turn do parameters, ..\n",
    "        self.net = nn.Sequential(nn.Linear(2, 2), nn.Sigmoid(), nn.Linear(2, 4), nn.Softmax())\n",
    "\n",
    "    # explicit forward though its only calling net..\n",
    "    # so when we call object(..) - it will be called.\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "# defining the network using sequantial\n",
    "# calling it on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "oPtrU6ZATEqW",
    "outputId": "1e29b72b-4361-414b-f8a3-2f8cc5218af1"
   },
   "outputs": [],
   "source": [
    "fn = FirstNetwork_v2()\n",
    "fit_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIZIKWepU479"
   },
   "outputs": [],
   "source": [
    "# more clean fit\n",
    "\n",
    "# inputs to fit?\n",
    "\n",
    "# (not doing book keeping here)\n",
    "\n",
    "\n",
    "def fit_v2(x, y, model, opt, loss_fn, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = loss_fn(model(x), y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZoElzFAVGWv",
    "outputId": "08b20d3f-60cf-4c7f-d258-d3081f7a2921"
   },
   "outputs": [],
   "source": [
    "fn = FirstNetwork_v2()\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "opt = optim.SGD(fn.parameters(), lr=1)\n",
    "\n",
    "fit_v2(X_train, Y_train, fn, opt, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2XrMMNpWHwG"
   },
   "source": [
    "this way of coding with a core **fit function**.  \n",
    "\n",
    "declare model seperately - the can play with hyperparameters,optimizers, etc. in the fit function. Its takes all those stuff as input.\n",
    "\n",
    "\n",
    "\n",
    "so far:\n",
    "\n",
    "**Step by step abstraction**\n",
    "\n",
    "eg: Parameters -> Linear -> sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r07oI4cSHbvE"
   },
   "source": [
    "- nn.functional\n",
    "  functions for nn (cross_entropy,..)\n",
    "- nn.Module class  \n",
    "  calling object like a fn calls forward  \n",
    "  - nn.Parameters - wrap all parameters with this - then can iterate over them to update them.\n",
    "  shorthand for updating parameters that way, and also setting all gradients to 0 after updating.\n",
    "  - nn.Linear - interanally do parameters,etc. make weights, biases,.. - easy way to do weighted sum.. \n",
    "  - nn.Sequential\n",
    "- optim\n",
    "\n",
    "\n",
    "\n",
    "- nn.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KITy39oMJJSD"
   },
   "source": [
    "### Running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jahIxW1XsBu",
    "outputId": "c058e45d-056c-4be1-a139-3897759b01d7"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# push data and model to the device.\n",
    "\n",
    "# data to gpu\n",
    "X_train=X_train.to(device)\n",
    "Y_train=Y_train.to(device)\n",
    "\n",
    "fn = FirstNetwork_v2()\n",
    "\n",
    "# model to gpu\n",
    "fn.to(device)\n",
    "\n",
    "# model has all the other tensors - weights, etc.. \n",
    "\n",
    "# only that much and everything is in gpu now.\n",
    "\n",
    "tic = time.time()\n",
    "print('Final loss', fit_v2(X_train, Y_train, fn, opt, loss_fn))\n",
    "toc = time.time()\n",
    "print('Time taken', toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYn7NST3YBXq"
   },
   "outputs": [],
   "source": [
    "# a larger model.\n",
    "\n",
    "\n",
    "class FirstNetwork_v3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 1024 * 4), nn.Sigmoid(), nn.Linear(1024 * 4, 4), nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jegpjj1TYDDa",
    "outputId": "8846a521-765c-4b1d-9408-38fc352726c9"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "X_train=X_train.to(device)\n",
    "Y_train=Y_train.to(device)\n",
    "fn = FirstNetwork_v3()\n",
    "fn.to(device)\n",
    "tic = time.time()\n",
    "print('Final loss', fit_v2(X_train, Y_train, fn, opt, loss_fn))\n",
    "toc = time.time()\n",
    "print('Time taken', toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxPb0Ip2YE3V",
    "outputId": "9610c5ca-c1c2-495a-f10c-880a430a3772"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "X_train=X_train.to(device)\n",
    "Y_train=Y_train.to(device)\n",
    "fn = FirstNetwork_v3()\n",
    "fn.to(device)\n",
    "tic = time.time()\n",
    "print('Final loss', fit_v2(X_train, Y_train, fn, opt, loss_fn))\n",
    "toc = time.time()\n",
    "print('Time taken', toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoSkLtD922OC"
   },
   "source": [
    "cuda could do it much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVZHQ3TJ2-Yq"
   },
   "source": [
    "powers of 2 - GPU architecture is orgaised that way. better performance if all work distributions are in powers of 2. also matrices allign with the memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmfUcyF537XE"
   },
   "source": [
    "**read docs of stuff.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-Te6q4s4AYa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
