{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qF-JNl6H7iRH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "\n",
    "import torchvision.transforms as transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "e531924f1b2245c3b3df58fded0ba7e7",
      "c9ff2d95dde74387b0eaaa7a5a042ce8",
      "47bfaa727f1642f786586b9644b25398",
      "73dae33c4c02436f8328541003939dd4",
      "a0b7f9fd74e84e6da344324e5ce00f16",
      "b55a985c4b0249728902e7f5e809adbb",
      "2971f89ab28343d89f60c49e91246d85",
      "ed72e1b1fc95400a87ae985f5f243e17",
      "5b3ae190359f489396ef599f4b3c90be",
      "1c215ed9810f4007a0335ef3696edf57",
      "61656e5e429048c7905494904f747d1f"
     ]
    },
    "id": "R0qHSVKb772v",
    "outputId": "a9b2cff4-8c31-43b7-b488-ad17e05b33dc"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='../data_place/cifar', \n",
    "                                        train=True, \n",
    "                                        download=False, \n",
    "                                        transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ll2_k-5c-3zO"
   },
   "outputs": [],
   "source": [
    "class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# in the dataset this will be in numbers.. class 0,1,2,.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SGPdCCPK-6pa"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, #dataset\n",
    "                                          batch_size=4, \n",
    "                                          shuffle=True)\n",
    "\n",
    "# shuffle = True => they are given randomly.\n",
    "# if we don't shuffle then sometimes the whole training course become repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator for dataloader\n",
    "data_iter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one batch\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WcJtpt56L66y"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks with only convolution layers(no activation..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-F4Riq7LhrL"
   },
   "source": [
    "### single convln layer - model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Conv2d is an inbuilt model-block-class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ibxp3KmkMBF3"
   },
   "outputs": [],
   "source": [
    "class OneCNNLayer(nn.Module): #inherit nn.Module\n",
    "\n",
    "    def __init__(self): \n",
    "        \n",
    "        super().__init__() #super with this class as arg??\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, \n",
    "                               padding=(1,1), \n",
    "                               stride=(2,2)) \n",
    "        # 3 - number of channels in input (kernel depth)\n",
    "        # 16 - number of output channels(number of kernels)\n",
    "        # 3 - kernel size. 3x3. can instead use a tupe (lxw) \n",
    "        # (can have rectangular kernels.. )\n",
    "        # if padding, stride are not mentioned, they will be taken as 0.\n",
    "        # along row, column\n",
    "\n",
    "        \n",
    "    def forward(self, XX):\n",
    "        XX = self.conv1(XX)\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYk2Sxm5MCe_"
   },
   "source": [
    "**define all network blocks (\"model-block objects\").. in init -> forward.. only calling..**   \n",
    "\"forward\" is necessary for \\_\\_call__ to work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1FWVAi1SNaqP"
   },
   "outputs": [],
   "source": [
    "one_cnn_layer = OneCNNLayer() # model_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference  \n",
    "\n",
    "model is a relation b/w an input- an ouput.  \n",
    "\n",
    "inference => get ouput, given input.  \n",
    "\n",
    "this would be something that we do, when we are using a trained model for some application.  \n",
    "\"trained\" => the model parameters will be optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMuKLDa4N9EF",
    "outputId": "7886c156-34fd-4b7b-ae53-122fe4fe480f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 16, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = one_cnn_layer(images) #calls forward()\n",
    "# because of __call__\n",
    "\n",
    "#here : do convoln and return the ouput channels\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cevVgflHN-bd"
   },
   "source": [
    "4 x 16 channels x 16 rows x 16 cols  \n",
    "\n",
    "batch size=4. doing convoln, etc. in parallel to n datapoints(images)  \n",
    "\n",
    "16 - number of ouput channels   \n",
    "one channel - 16x16   \n",
    "\n",
    "with what all kernels?  \n",
    "those which get inintialised(interanlly) when we do nn.Conv2d.   \n",
    "\n",
    "**kernel => weights (parameters)**    \n",
    "model has parameters, forward.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vp-OCczWPAQu",
    "outputId": "ac20577d-7141-4bd8-9ebb-f35fe7de416c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 3, 3])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# kernel = parameters\n",
    "for param in one_cnn_layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can call model_object.parameters() - for any model_block_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7KgLyBoPAhL"
   },
   "source": [
    "2 parameteres internally made(wrapped in Parameter()).\n",
    "\n",
    "one - 16 3x3x3 kernels  \n",
    "another - 16 biases  \n",
    "one bias for each kernel.  \n",
    "\n",
    "**internally many things are happening**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### showing second layer ouput(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXY9ssmYQIBB",
    "outputId": "30798d77-d5d0-481d-f956-4ca66ec43088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# plot one channel of the outputs of convoln.\n",
    "# output assigned to 'out'\n",
    "\n",
    "# out -> 4, 16, 16, 16 (batch, channel, x, y)\n",
    "\n",
    "out0 = out[0, 0, :, :].detach().numpy() \n",
    "# 0th batch, 0th channel\n",
    "print(out0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### detach\n",
    "\n",
    "tensor.detach() - Returns a new Tensor **(share the storage with original), detached from the current graph.**  \n",
    "It detaches the output from the computational graph; So no gradient will be backpropagated along this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# share memeory\n",
    "aa = torch.ones(2,2)\n",
    "bb = aa.detach()\n",
    "bb[0,0] = 2\n",
    "\n",
    "print(aa)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between a torch.tensor and np.ndarray:  \n",
    "\n",
    "While both objects are used to store n-dimensional matrices (aka \"Tensors\"), torch.tensors has an additional \"layer\" - which is storing the computational graph leading to the associated n-dimensional matrix.\n",
    "\n",
    "So, if you are only interested in efficient and easy way to perform mathematical operations on matrices np.ndarray or torch.tensor can be used interchangeably.\n",
    "\n",
    "However, torch.tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but (and more importantly) the computational graph leading to these values. This computational graph is then used (using the chain rule of derivatives) to compute the derivative of the loss function w.r.t each of the independent variables used to compute the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor.detach().numpy()\n",
    "\n",
    "As mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "8wQGiZgCQ4J8",
    "outputId": "ccc4b332-1ac7-4081-bdd1-5a8588069e83"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out0) # not transposing, etc. as its not rgb. only a single channel like a greyscale\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWsMwYDfReDm"
   },
   "source": [
    "some features of the original image can be seen here.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-puMbw7Sd6I"
   },
   "source": [
    "### 2 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-9ejsh0OSnBO"
   },
   "outputs": [],
   "source": [
    "class TwoCNNLayers(nn.Module):\n",
    "    def __init__(self): \n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # two convoln layers\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3),   # (N, 3, 32, 32) -> (N, 8, 30, 30)\n",
    "            nn.Conv2d(8, 16, 3)   # (N, 8, 30, 30) -> (N, 16, 28, 28)\n",
    "        )\n",
    "        # first layer - out 8 channels\n",
    "        # second layer (after it) - input should be 8 channels.\n",
    "        \n",
    "    def forward(self, XX):\n",
    "        XX = self.model(XX)\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DREXraoT8VD"
   },
   "source": [
    "(N, 3, 32, 32) **tracking dimensions**.  \n",
    "**N = batch size**, etc..   \n",
    "\n",
    "**while making model - think about one datapoint only. Batch, etc. taken care automatically by broadcasting, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6gGSlbaTDFg",
    "outputId": "6de2db09-ef50-494f-92c6-49faa443fef6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "two_cnn_layers = TwoCNNLayers()\n",
    "out = two_cnn_layers(images)\n",
    "\n",
    "# now out has results after the two layers.\n",
    "\n",
    "out.shape\n",
    "\n",
    "# get the shape as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAgRjKZjTSmP"
   },
   "source": [
    "### Convoln layers with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "T3BmUr07UucX"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),          # (N, 3, 32, 32) -> (N, 6, 28, 28)\n",
    "            \n",
    "            # 2x2 average pooling kernel\n",
    "            # using stride = 2 so that no overlap\n",
    "            # this average pool is not the avg pool in GoogleNet.\n",
    "            # this is avg instead of max.\n",
    "            \n",
    "            nn.AvgPool2d(2, stride=2),   # (N, 6, 28, 28) -> (N, 6, 14, 14)\n",
    "            nn.Conv2d(6, 16, 5),         # (N, 6, 14, 14) -> (N, 16, 10, 10)\n",
    "            nn.AvgPool2d(2, stride=2)    # (N, 16, 10, 10) -> (N, 16, 5, 5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, XX):\n",
    "        XX = self.model(XX)\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL1RsChKVIbz",
    "outputId": "e4326fa6-d1c6-4539-abfb-bb9eb8c7865d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 5, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNN()\n",
    "out = cnn_model(images)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgHuor5dWpV2"
   },
   "source": [
    "above : just doing convoln - one after another - no activation in b/w. as it was not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c215ed9810f4007a0335ef3696edf57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2971f89ab28343d89f60c49e91246d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47bfaa727f1642f786586b9644b25398": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed72e1b1fc95400a87ae985f5f243e17",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b3ae190359f489396ef599f4b3c90be",
      "value": 170498071
     }
    },
    "5b3ae190359f489396ef599f4b3c90be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61656e5e429048c7905494904f747d1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73dae33c4c02436f8328541003939dd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c215ed9810f4007a0335ef3696edf57",
      "placeholder": "​",
      "style": "IPY_MODEL_61656e5e429048c7905494904f747d1f",
      "value": " 170498071/170498071 [00:01&lt;00:00, 110934476.60it/s]"
     }
    },
    "a0b7f9fd74e84e6da344324e5ce00f16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b55a985c4b0249728902e7f5e809adbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ff2d95dde74387b0eaaa7a5a042ce8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b55a985c4b0249728902e7f5e809adbb",
      "placeholder": "​",
      "style": "IPY_MODEL_2971f89ab28343d89f60c49e91246d85",
      "value": "100%"
     }
    },
    "e531924f1b2245c3b3df58fded0ba7e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9ff2d95dde74387b0eaaa7a5a042ce8",
       "IPY_MODEL_47bfaa727f1642f786586b9644b25398",
       "IPY_MODEL_73dae33c4c02436f8328541003939dd4"
      ],
      "layout": "IPY_MODEL_a0b7f9fd74e84e6da344324e5ce00f16"
     }
    },
    "ed72e1b1fc95400a87ae985f5f243e17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
