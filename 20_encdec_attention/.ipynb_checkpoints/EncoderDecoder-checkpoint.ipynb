{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p84NpJGk0f_j"
   },
   "source": [
    "Task : Transliteration   \n",
    "\n",
    "**sequence to sequence generation**  \n",
    "\n",
    "processing xml files to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpuvHS0mxwCd"
   },
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKfe2BV7IWit"
   },
   "source": [
    "xml files - one training(13937), one testing(1000).\n",
    "\n",
    "xml - heirarchical data format.  \n",
    "top - root node  \n",
    "in that elements.. \n",
    "\n",
    "id - unique id  \n",
    "sorce name  \n",
    "target name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYrAa5laSptM"
   },
   "source": [
    "### Alphabets Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-a04ZKx7Sh-J",
    "outputId": "022d306e-eeb9-428b-8340-f884e43034c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
     ]
    }
   ],
   "source": [
    "# index for characters\n",
    "# for one hot, etc.. \n",
    "\n",
    "# transliteration - not specific to uppper, lower case.\n",
    "# all converted to one case.. here upper case.\n",
    "\n",
    "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "pad_char = '-PAD-'\n",
    "\n",
    "# dictionary with key = alphabet, value = index\n",
    "eng_alpha2index = {pad_char: 0}\n",
    "for index, alpha in enumerate(eng_alphabets):\n",
    "    eng_alpha2index[alpha] = index+1\n",
    "\n",
    "print(eng_alpha2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPSZsy1kXd9w",
    "outputId": "6974bbbe-8885-4298-8de8-b4578ba3f963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
     ]
    }
   ],
   "source": [
    "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
    "# unicode (binary for letters..) for hindi letters.\n",
    "# adayaalams - different character.. \n",
    "# aksharam with adayalm will be thus 2 characters.\n",
    "\n",
    "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "hindi_alphabet_size = len(hindi_alphabets)\n",
    "\n",
    "# dictionary with key = alphabet, value = index\n",
    "hindi_alpha2index = {pad_char: 0}\n",
    "for index, alpha in enumerate(hindi_alphabets):\n",
    "    hindi_alpha2index[alpha] = index+1\n",
    "\n",
    "print(hindi_alpha2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSw1SMZmx9A3"
   },
   "source": [
    "### Helper functions for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OcS6ByndOxrC"
   },
   "outputs": [],
   "source": [
    "import re # regex\n",
    "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "# Remove all non-English letters\n",
    "def cleanEnglishVocab(line):\n",
    "    line = line.replace('-', ' ').replace(',', ' ').upper()\n",
    "    line = non_eng_letters_regex.sub('', line)\n",
    "    return line.split()\n",
    "\n",
    "# Remove all non-Hindi letters\n",
    "# can do regex effectively in hindi.\n",
    "# so loop and do manually\n",
    "def cleanHindiVocab(line):\n",
    "    line = line.replace('-', ' ').replace(',', ' ')\n",
    "    cleaned_line = ''\n",
    "    for char in line:\n",
    "        if char in hindi_alpha2index or char == ' ':\n",
    "            cleaned_line += char\n",
    "    return cleaned_line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob3F9Dh4PChB"
   },
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M721IiOzMUAY"
   },
   "source": [
    "in a DL project - there is a component of software engg. To process raw data.. and bring it into a form that can be passed to a network. (**pre-processing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader class\n",
    "- inherinting from torch.utils.data.Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KGSeoMGg0FTy"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# extending dataset class of pytorch \n",
    "# process a given data.. and give workable 'dataset'\n",
    "class TransliterationDataLoader(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # read file - corresponding eng, hindi words\n",
    "        self.eng_words, self.hindi_words = self.readXmlDataset(filename, cleanHindiVocab)\n",
    "        \n",
    "        # shuffled indices\n",
    "        self.shuffle_indices = list(range(len(self.eng_words))) # list of indices.\n",
    "        random.shuffle(self.shuffle_indices) # list of shuffled indices.\n",
    "        \n",
    "        self.shuffle_start_index = 0  # maintain this pointer - start from this give next b points.\n",
    "        # update this pointer.\n",
    "        # shuffle everything - then give batch by batch using a pointer.\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eng_words)\n",
    "    \n",
    "    # magic function that allows for square bracket indexing\n",
    "    def __getitem__(self, idx): # word pair at index.\n",
    "        return self.eng_words[idx], self.hindi_words[idx]\n",
    "    \n",
    "    def readXmlDataset(self, filename, lang_vocab_cleaner):\n",
    "\n",
    "        # parse file\n",
    "        transliterationCorpus = ET.parse(filename).getroot() # gives root.\n",
    "        # here root: \"TransliterationCorpus\" (first tag)\n",
    "        # all rest are childrens\n",
    "        lang1_words = []\n",
    "        lang2_words = []\n",
    "\n",
    "        # root - iteratable \n",
    "        # like a nested list - (heirarchical)\n",
    "        # can iterate through the children\n",
    "        for line in transliterationCorpus:\n",
    "            wordlist1 = cleanEnglishVocab(line[0].text)\n",
    "            wordlist2 = lang_vocab_cleaner(line[1].text)\n",
    "\n",
    "            # Skip noisy data\n",
    "            if len(wordlist1) != len(wordlist2): # words (not letters)\n",
    "                print('Skipping: ', line[0].text, ' - ', line[1].text)\n",
    "                continue\n",
    "\n",
    "            for word in wordlist1:\n",
    "                lang1_words.append(word)\n",
    "            for word in wordlist2:\n",
    "                lang2_words.append(word)\n",
    "\n",
    "        return lang1_words, lang2_words\n",
    "    \n",
    "    def get_random_sample(self): # random datapoint\n",
    "    # using get_item with random index.\n",
    "        return self.__getitem__(np.random.randint(len(self.eng_words)))\n",
    "    \n",
    "    # get a batch of size=batch_size from that array.\n",
    "    def get_batch_from_array(self, batch_size, array):\n",
    "        end = self.shuffle_start_index + batch_size\n",
    "        batch = []\n",
    "\n",
    "        # if overflow - loop to first - rest of the points from the beginning.\n",
    "        if end >= len(self.eng_words):\n",
    "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
    "            end = len(self.eng_words)\n",
    "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n",
    "    \n",
    "    # both arrays - batch.\n",
    "    def get_batch(self, batch_size, postprocess = True):\n",
    "        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
    "        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n",
    "        self.shuffle_start_index += batch_size + 1 #update index.\n",
    "        \n",
    "        # Reshuffle if 1 epoch is complete\n",
    "        if self.shuffle_start_index >= len(self.eng_words):\n",
    "            random.shuffle(self.shuffle_indices)\n",
    "            self.shuffle_start_index = 0\n",
    "            \n",
    "        return eng_batch, hindi_batch\n",
    "\n",
    "\n",
    "\n",
    "# this is one way to implement a dataloader(not the most efficient way)\n",
    "# get item, get batch, .. such stuff which can be used w/o having to worry about whats in.\n",
    "# encapsulating the dataloading part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "-FCCi-SerZS-",
    "outputId": "9d47c9b2-dcb0-42ba-9405-ab71cf00ed85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping:  BARHARWA JUNCTION  -  बरहरवा\n",
      "Skipping:  STATE BNK TR  -  स्टेट बैंक ऑफ त्रावणकोर\n",
      "Skipping:  SOUTH ARLINGTON CHURCH OF CHRIST  -  साउथ अर्लिंग्टन\n",
      "Skipping:  KING EDWARD VII  -  किंग एडवर्ड\n",
      "Skipping:  DIBANG VALLEY  -  दिबंगवैली\n",
      "Skipping:  ORDER OF VASA  -  ऑडर ऑफ़ द वासा\n",
      "Skipping:  AZAMNAGAR ROAD  -  आज़मनगर\n",
      "Skipping:  CAPE TOWN  -  केपटाउन\n",
      "Skipping:  NEW ZEALAND  -  न्यूज़ीलैंड\n",
      "Skipping:  SEA OF THE HEBRIDES  -  सी ऑफ हरब्रिड्‍स\n",
      "Skipping:  RAMCOIND  -  राम्को इंड\n",
      "Skipping:  KELVINGROVE ART GALLERY AND MUSEUM  -  केल्व‍िनग्रोव आर्ट एण्ड म्युज़ियम\n",
      "Skipping:  AUSTRALIAN NATIONAL UNIVERSITY  -  ऑस्ट्रेलियननेशनल यूनिवर्सिटी\n",
      "Skipping:  JAHAN AARA  -  जहाँआरा\n",
      "Skipping:  NAVABHARAT FERRO ALLOYS  -  नव भारत फ़ैरो अलॉय\n",
      "Skipping:  RAMA LINGESHWARA  -  रामालिंगेश्वर\n",
      "Skipping:  FAKHRUN NISA  -  फखरुन्निसा\n",
      "Skipping:  REDIFF.COM INDIA LIMITED  -  रेडिफ़ डॉट कॉम इंडिया लिमिटेड\n",
      "Skipping:  OMKARNATH THAKUR  -  ओंकार नाथ ठाकुर\n",
      "Skipping:  OPENTV  -  ओपन टीवी\n",
      "Skipping:  ENVOY COMMUNICATIONS GROUP  -  एन्वॉय कम्युनिकेशंस\n",
      "Skipping:  WAR OF THE HOLY LEAGUE  -  वार ऑफ होली लीग\n",
      "Skipping:  VAPARAISO CHURCH OF CHRIST  -  व्हापरासिओ\n",
      "Skipping:  PARIS CHARLES DE GAULLE  -  पेरिस रॉसे चार्ल्स डे ग्यूले\n",
      "Skipping:  PARKWAY APOSTOLIC  -  पार्क वे अपोस्टोलिक\n",
      "Skipping:  MAUNA LOA  -  मौनालोआ\n",
      "Skipping:  MASS MUTUAL LIFE  -  मास म्युच्युअल लाइफ़ इंश्योरेंस\n",
      "Skipping:  STATS CHIPPAC  -  स्टेट्सचिपपैक\n",
      "Skipping:  NEWFOUNDLAND  -  न्यू फाउंडलैंड\n",
      "Skipping:  LONDONHEATHROW  -  लंदन हीथ्रो\n",
      "Skipping:  RETALIX  -  रेटालिक्स लि.\n",
      "Skipping:  SRISAILAM  -  श्री शैलम\n",
      "Skipping:  KARA-KUM  -  काराकुम\n",
      "Skipping:  WIND RIVER  -  विंडरिवर\n",
      "Skipping:  NETAJI SUBHASH CHANDRA BOSE  -  नेताजी सुभाषचंद्र बोस\n",
      "Skipping:  ROCKBROOK UNITED  -  रॉकब्रुक यूनाइटेड मेथोडिस्ट\n",
      "Skipping:  WALTER SCOTT  -  वॉल्टरस्कॉट\n",
      "Skipping:  COLOURPLUS FASHIONS  -  कलर प्लस फ़ैशन्स\n",
      "Skipping:  BAL KRISHNA  -  बालकृष्णा\n"
     ]
    }
   ],
   "source": [
    "train_data = TransliterationDataLoader('NEWS2012-Training-EnHi-13937.xml')\n",
    "test_data = TransliterationDataLoader('NEWS2012-Ref-EnHi-1000.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZO8RaWDUQYs"
   },
   "source": [
    "errors in data - as can be seen above. good practice to go through the data.. see what all errors are there.. and dealing with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l-iaCVdx5Ez"
   },
   "source": [
    "#### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "IjY06ghEx76b",
    "outputId": "279948f9-59ec-477a-fd44-8ddf8c96f368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size:\t 20543\n",
      "Test Set Size:\t 1000\n",
      "\n",
      "Sample data from train-set:\n",
      "WARNE - वार्न\n",
      "DIBBA - डिब्बा\n",
      "KI - की\n",
      "BHATKHEDE - भातखंडे\n",
      "MERU - मेरु\n",
      "HAMARI - हमारी\n",
      "NORTH - नॉर्थ\n",
      "MUHAAYEE - मुहायी\n",
      "PETRIE - पेट्री\n",
      "BANDRA - बांद्रा\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set Size:\\t\", len(train_data))\n",
    "print(\"Test Set Size:\\t\", len(test_data))\n",
    "\n",
    "print('\\nSample data from train-set:')\n",
    "for i in range(10):\n",
    "    eng, hindi = train_data.get_random_sample()\n",
    "    print(eng + ' - ' + hindi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpDP1_KYZIkv"
   },
   "source": [
    "### Encoding the words\n",
    "\n",
    "This encoding is not the encoding in the n/w part. \n",
    "\n",
    "making them tensor ... to be passed as inputs.   \n",
    "\n",
    "need **numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JE3at5C7Sy5F"
   },
   "outputs": [],
   "source": [
    "# encode input word(english) as one hot\n",
    "# each letter - one hot\n",
    "def word_rep(word, letter2index, device = 'cpu'):\n",
    "    \n",
    "    rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
    "    # dimension - as said earlier.. - padded... tensor.. \n",
    "    \n",
    "    for letter_index, letter in enumerate(word):\n",
    "        pos = letter2index[letter]\n",
    "        rep[letter_index][0][pos] = 1\n",
    "    pad_pos = letter2index[pad_char]\n",
    "    rep[letter_index+1][0][pad_pos] = 1\n",
    "    return rep\n",
    "\n",
    "# encode ground truth (hindi word)\n",
    "# basically classification\n",
    "# ouput sequence - seq of class labels\n",
    "# ie, one element is a class label (index corresponding to the letter)\n",
    "def gt_rep(word, letter2index, device = 'cpu'):\n",
    "    gt_rep = torch.zeros([len(word)+1, 1], dtype=torch.long).to(device)\n",
    "    for letter_index, letter in enumerate(word):\n",
    "        pos = letter2index[letter]\n",
    "        gt_rep[letter_index][0] = pos\n",
    "    gt_rep[letter_index+1][0] = letter2index[pad_char]\n",
    "    return gt_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "-yE3jToOrfzP",
    "outputId": "f06ed7e4-97bb-4780-cb2f-1327226a7e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAWAN tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "eng, hindi = train_data.get_random_sample()\n",
    "eng_rep = word_rep(eng, eng_alpha2index)\n",
    "print(eng,\"\\n\", eng_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "uMcDjIberhc3",
    "outputId": "30270e8a-733a-432e-8992-34330ca44e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "जवान tensor([[29],\n",
      "        [54],\n",
      "        [63],\n",
      "        [41],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "hindi_gt = gt_rep(hindi, hindi_alpha2index)\n",
    "print(hindi, \"\\n\",  hindi_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrC3tSnm4rUk"
   },
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4OgdZ_DVVC5"
   },
   "source": [
    "### Encoder-Decoder (using GRU)\n",
    "\n",
    "- encoder part - gru\n",
    "- decoder part - gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single call (w/o looping) for encoder - **batch of sequences passed**   \n",
    "\n",
    "manually looping in decoder part - **passing sequence one by one**\n",
    "\n",
    "return : output is a tensor with all the outputs(output at each step)   \n",
    "hidden is a single vector with last hidden state. it gets over-written in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6w8ffT3w4lkK"
   },
   "outputs": [],
   "source": [
    "MAX_OUTPUT_CHARS = 30\n",
    "class Transliteration_EncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # there is a state vector in encoder and one in decoder.\n",
    "        # here we are taking both to be of same dim.(need not be so.)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # BLOCKS:\n",
    "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
    "        self.decoder_rnn_cell = nn.GRU(output_size, hidden_size)\n",
    "        # input to decoder RNN is yi(ouput of prev RNN cell itself)\n",
    "        # thus input dim is output dim.\n",
    "        \n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        # verbose - good practice for understanding, debugging, .. \n",
    "        \n",
    "    def forward(self, input_, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
    "        \n",
    "        # ENCODER:\n",
    "        out, hidden = self.encoder_rnn_cell(input_)\n",
    "        # one go - input as a tensor.\n",
    "        # internally it happens in sequence- step by step.\n",
    "        \n",
    "        # printing shapes of ouput, hidden\n",
    "        if self.verbose:\n",
    "            print('Encoder input', input_.shape)\n",
    "            print('Encoder output', out.shape)\n",
    "            print('Encoder hidden', hidden.shape)\n",
    "        \n",
    "        # hidden state is higher dim than input. \n",
    "        # we want it to learn more stuff in the input. \n",
    "        \n",
    "        # output is a tensor with all the outputs(output at each step)\n",
    "        # hidden is a single vector with last hidden state. it gets over-written in the steps.\n",
    "        \n",
    "        # DECODER:\n",
    "        decoder_state = hidden # S_0 = h_T\n",
    "        # here encoder and decoder state size is same - therefore above can be done direclty.\n",
    "        # else some linear layer in between to do the transposition\n",
    "        decoder_input = torch.zeros(1, 1, self.output_size).to(device) # y_0 <sos> -ish.\n",
    "        # input to decoder same dimension as output.\n",
    "        outputs = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Decoder state', decoder_state.shape)\n",
    "            print('Decoder input', decoder_input.shape)\n",
    "            \n",
    "        \n",
    "        # not invoking decoder in a single call (like the encoder.)\n",
    "        # but how does the program know if we are looping or not(??)\n",
    "        # BASED ON THE INPUT\n",
    "        # INTERNAL IMPLEMENTATION : LOOP OVER INPUT 'SEQUENCE'\n",
    "        \n",
    "        # LOOP:\n",
    "        \n",
    "        for i in range(max_output_chars):\n",
    "            \n",
    "            # y_1, s_1 from y_0, s_0\n",
    "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Decoder intermediate output', out.shape)\n",
    "            \n",
    "            # output from state\n",
    "            out = self.h2o(decoder_state)\n",
    "            out = self.softmax(out)\n",
    "            \n",
    "            outputs.append(out.view(1, -1)) # list of outputs - this is returned.\n",
    "            \n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Decoder output', out.shape)\n",
    "                self.verbose = False\n",
    "            \n",
    "            max_idx = torch.argmax(out, 2, keepdim=True) # index of max\n",
    "            \n",
    "            # if ground truth - ie, if its mentioned pass ground truth as input to next step\n",
    "            # then one hot with the ground truth index and pass that.\n",
    "            # else one hot with max-index\n",
    "            if not ground_truth is None:\n",
    "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
    "                \n",
    "            # make one hot vector out of the index\n",
    "            one_hot = torch.FloatTensor(out.shape).to(device)\n",
    "            one_hot.zero_()\n",
    "            one_hot.scatter_(2, max_idx, 1)\n",
    "            \n",
    "            # any function .. in the computation graph.\n",
    "            # back prop through all.\n",
    "            # sometimes we don't want BP through some.\n",
    "            # eg - ouput of one step passes as input ot next step. - this is also a link\\\n",
    "            # but we don't want to BP through that.\n",
    "            decoder_input = one_hot.detach()\n",
    "            # detach() -> don't pass anymore grad through this tensor.\n",
    "            # ie, not part of the computational graph for gradient.\n",
    "            \n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, word,max_output_chars, device='cpu'):\n",
    "    model.eval().to(device)\n",
    "    word_ohe = word_rep(word, eng_alpha2index)\n",
    "    output = model(word_ohe, max_output_chars)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Cra9toTiOoPm"
   },
   "outputs": [],
   "source": [
    "encdec = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "v4zaJq2pOrM8",
    "outputId": "8f63335b-fa7d-401f-d4db-872dc20c508d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input torch.Size([6, 1, 27])\n",
      "Encoder output torch.Size([6, 1, 256])\n",
      "Encoder hidden torch.Size([1, 1, 256])\n",
      "Decoder state torch.Size([1, 1, 256])\n",
      "Decoder input torch.Size([1, 1, 129])\n",
      "Decoder intermediate output torch.Size([1, 1, 256])\n",
      "Decoder output torch.Size([1, 1, 129])\n"
     ]
    }
   ],
   "source": [
    "out = infer(encdec, 'INDIA', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### untrained infrerence\n",
    "#### its good to see if the forward pass is working even before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "1_pdzBmQOsjO",
    "outputId": "b6fb8423-1f01-4f3c-f1b9-f5dd9a8fb752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "torch.Size([1, 129]) ॎ\n",
      "torch.Size([1, 129]) ६\n",
      "torch.Size([1, 129]) घ\n",
      "torch.Size([1, 129]) घ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n",
      "torch.Size([1, 129]) ॸ\n"
     ]
    }
   ],
   "source": [
    "print(len(out))\n",
    "for i in range(len(out)):\n",
    "    print(out[i].shape, list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEg49N9e7oTY"
   },
   "source": [
    "# Encoder-Decoder with Attention \n",
    "\n",
    "\n",
    "weighted combination of all encoder hidden states.\n",
    "\n",
    "but if we run encoder in one go - with tensor input - then hidden state is getting overwritten (only the last hidden state is returned)     \n",
    "but for below we need all the hidden states.   \n",
    "\n",
    "Here - we use the \"ouputs\" (batch) returned instead of hidden_state.   \n",
    "(a choice that we make)   \n",
    "\n",
    "loop in the decoder part - and do the extra attention stuff needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If batch of input is passed to gru - it runs the loop internally and returns the last state, and a batch of ouput (at-each-seq-step).  \n",
    "If we need all the states also - then manually loop - passing one-in-the-sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8z-1QDAz8F_d"
   },
   "outputs": [],
   "source": [
    "class Transliteration_EncoderDecoder_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "        # this is like option 2 of encoder decoder. concatenating encoding at each step\n",
    "        # attention - concatenating 'refined encoding' at each step\n",
    "        # therefore input to GRU cell - twice the size.\n",
    "        self.decoder_rnn_cell = nn.GRU(hidden_size*2, hidden_size)\n",
    "        \n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        # additional layers.\n",
    "        # Ws, Uh in attention 'function' - as linear layers.\n",
    "        # similarly Vatt.\n",
    "        self.U = nn.Linear(self.hidden_size, self.hidden_size) # Uatt\n",
    "        self.W = nn.Linear(self.hidden_size, self.hidden_size) # Watt\n",
    "        self.attn = nn.Linear(self.hidden_size, 1) # Vatt\n",
    "        \n",
    "        # decoder ouput to state (to match dimension.)  (??)\n",
    "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   \n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
    "        \n",
    "        # ENCODER: \n",
    "        \n",
    "        # get only one hidden state. - but tensor of ouputs\n",
    "        # we need all states for attention\n",
    "        # instead of states - use the 'ouputs' of encoder. - this is a choice we make.\n",
    "        # as all of them are available as a tensor.\n",
    "        encoder_outputs, hidden = self.encoder_rnn_cell(input)\n",
    "        encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Encoder output', encoder_outputs.shape)\n",
    "        \n",
    "        # DECODER : \n",
    "        \n",
    "        decoder_state = hidden # S_0 = h_T\n",
    "        decoder_input = torch.zeros(1, 1, self.output_size).to(device) # first decoder unit input\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        U = self.U(encoder_outputs) \n",
    "        # function that takes encoder outputs(which we are using instead of encoder states)\n",
    "        # and pass through the U layer ..\n",
    "        ## because that part in the attention function - not chainging in decoder steps.\n",
    "        # h_i are fixed and each multiplied by U - so do that(once) and keep it.. to be used in decoder stage.\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Decoder state', decoder_state.shape)\n",
    "            print('Decoder intermediate input', decoder_input.shape)\n",
    "            print('U * Encoder output', U.shape)\n",
    "        \n",
    "        # going step by step (loop) in decoder part.\n",
    "        \n",
    "        for i in range(max_output_chars):\n",
    "            \n",
    "            # get the refined encoding.\n",
    "            \n",
    "            W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0], 1)) # W * S_t-1\n",
    "            # repeat - tile.. to make same size.\n",
    "            \n",
    "            V = self.attn(torch.tanh(U + W)) # e_tj\n",
    "            \n",
    "            attn_weights = F.softmax(V.view(1, -1), dim = 1)  # alpha_tj\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('W * Decoder state', W.shape)\n",
    "                print('V', V.shape)\n",
    "                print('Attn', attn_weights.shape)\n",
    "            \n",
    "            # weighted combination of encoder ouputs(instead of states, here), with alphas\n",
    "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "            \n",
    "            \n",
    "            # decoder input- concatenation of 'refined encoding' just found, input(decoder_ouput of prev unit)\n",
    "            \n",
    "            # not decoder input direcltly, but after trasnforming to same size.\n",
    "            embedding = self.out2hidden(decoder_input) # linear layer on decoder input \n",
    "            # to make it the same size as the refined encoding.. so that same contribution in terms of size..\n",
    "            # that layer for this - parameters - also learned.\n",
    "            \n",
    "            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Attn LC', attn_applied.shape)\n",
    "                print('Decoder input', decoder_input.shape)            \n",
    "            \n",
    "            \n",
    "            # here is where we are finding the decoder unit output \n",
    "            # first we find the decoder input - concatening decoder_ouput(transformed) and refined_encoding.\n",
    "            # done above.\n",
    "            # then pass that input to the decoder cell.\n",
    "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Decoder intermediate output', out.shape)\n",
    "                \n",
    "            out = self.h2o(decoder_state)\n",
    "            out = self.softmax(out)\n",
    "            outputs.append(out.view(1, -1))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Decoder output', out.shape)\n",
    "                self.verbose = False\n",
    "            \n",
    "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
    "            if not ground_truth is None:\n",
    "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
    "            one_hot = torch.zeros(out.shape, device=device)\n",
    "            one_hot.scatter_(2, max_idx, 1) \n",
    "            \n",
    "            decoder_input = one_hot.detach()\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PMD3zjdJO0Oj"
   },
   "outputs": [],
   "source": [
    "net_attn = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### untrained inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "YoiQwbntO5UH",
    "outputId": "30e5ee26-fe66-426b-e2b6-5385c6c723f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output torch.Size([6, 256])\n",
      "Decoder state torch.Size([1, 1, 256])\n",
      "Decoder intermediate input torch.Size([1, 1, 129])\n",
      "U * Encoder output torch.Size([6, 256])\n",
      "W * Decoder state torch.Size([6, 256])\n",
      "V torch.Size([6, 1])\n",
      "Attn torch.Size([1, 6])\n",
      "Attn LC torch.Size([1, 1, 256])\n",
      "Decoder input torch.Size([1, 1, 512])\n",
      "Decoder intermediate output torch.Size([1, 1, 256])\n",
      "Decoder output torch.Size([1, 1, 129])\n"
     ]
    }
   ],
   "source": [
    "out = infer(net_attn, 'INDIA', 30)\n",
    "\n",
    "# printing dimensions of all stuff.. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "K9WSPgzlO6k8",
    "outputId": "55e23d81-96e5-456d-eb0b-eb470bfd3531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "torch.Size([1, 129]) ि\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) ध\n",
      "torch.Size([1, 129]) च\n",
      "torch.Size([1, 129]) ध\n"
     ]
    }
   ],
   "source": [
    "print(len(out))\n",
    "for i in range(len(out)):\n",
    "    print(out[i].shape, list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyE2tSnmAW6x"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H893cimDtTUE"
   },
   "source": [
    "### Train one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m804jsH7AXSV"
   },
   "outputs": [],
   "source": [
    "# train one batch\n",
    "\n",
    "def train_batch(model, batch_size, loss_fn, opt, teacher_force = False, device = 'cpu'):\n",
    "    \n",
    "    # teacher force - whether pass previous output(one_hot) or ground truth as the next input.\n",
    "    \n",
    "    # model\n",
    "    model.train().to(device)\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # data\n",
    "    eng_batch, hindi_batch = train_data.get_batch(batch_size) # get a batch from dataloader.\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        input = word_rep(eng_batch[i], eng_alpha2index, device)\n",
    "        gt = gt_rep(hindi_batch[i], hindi_alpha2index, device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(input, gt.shape[0], device, ground_truth = gt if teacher_force else None) \n",
    "        # note teacher force above.\n",
    "        \n",
    "        for index, output in enumerate(outputs):\n",
    "            # loss\n",
    "            loss = loss_fn(output, gt[index]) / batch_size\n",
    "            \n",
    "            # compute gradient\n",
    "            loss.backward(retain_graph = True) \n",
    "            # retain graph - there is more backward - and all grads be added.\n",
    "            # maintains all necessary connections in the computation graph to enable BP.\n",
    "            total_loss += loss\n",
    "    \n",
    "    # update parameters\n",
    "    opt.step()\n",
    "    \n",
    "    return total_loss/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-eZaBxstWz9"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Rjto129ssrpr"
   },
   "outputs": [],
   "source": [
    "def train(model, batch_size = 10, n_batches = 100, lr = 0.01, momentum = 0.9, display_freq=5, device = 'cpu'):\n",
    "    \n",
    "    # model\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # loss function\n",
    "    loss_fn = nn.NLLLoss(ignore_index = -1)\n",
    "    \n",
    "    # optimizer\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    teacher_force_upto = n_batches//3 # pass ground truth upto first third batch. then pass output of prev step.\n",
    "    \n",
    "    loss_arr = np.zeros(n_batches + 1)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        loss_arr[i+1] = (loss_arr[i]*i + train_batch(model, \n",
    "                                                     batch_size, \n",
    "                                                     loss_fn, opt, \n",
    "                                                     teacher_force = i<teacher_force_upto, \n",
    "                                                     device = device ))/(i + 1)\n",
    "        \n",
    "        # note teacher force condition.. \n",
    "        \n",
    "        if i%display_freq == display_freq-1:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print('Iteration', i, 'Loss', loss_arr[i])\n",
    "            plt.figure()\n",
    "            plt.plot(loss_arr[1:i], '-*')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "            print('\\n\\n')\n",
    "            \n",
    "    torch.save(model, 'model.pt')\n",
    "    return loss_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZY6RvqLtdX8"
   },
   "source": [
    "### Training without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1oQ3ZIWvtjfN"
   },
   "outputs": [],
   "source": [
    "encdec = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "E6LjVKQfoVMU",
    "outputId": "9b3f7f5a-b98b-4203-d137-1780ee362fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1999 Loss 0.186558797955513\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.52941817, 0.51248622, ..., 0.1866013 , 0.1865588 ,\n",
       "       0.18651748])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(encdec, batch_size = 64, n_batches=2000, lr=0.001, display_freq=10, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GM1Tj20omMi1"
   },
   "source": [
    "### Training with Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lxFLBqW1Ip4v"
   },
   "outputs": [],
   "source": [
    "net_att = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "tdRpJUXNIwuv",
    "outputId": "6e8ed733-4b95-4eae-ac09-be4f6a905d18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1039 Loss 0.18075764179229736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_att\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_gpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mtrain_setup\u001b[0;34m(net, lr, n_batches, batch_size, momentum, display_freq, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m loss_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_batches \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[0;32m---> 11\u001b[0m     loss_arr[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (loss_arr[i]\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m+\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_force\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43mteacher_force_upto\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m/\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39mdisplay_freq \u001b[38;5;241m==\u001b[39m display_freq\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m         clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(net, opt, criterion, batch_size, device, teacher_force)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs):\n\u001b[1;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, gt[index]) \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     19\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = train(net_att, batch_size = 64, n_batches=2000, lr=0.001, display_freq=10, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with attention - better performance .\n",
    "\n",
    "Extra parameters.. (+ they need to be trained.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05F1-FwX6YVZ"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3TWC7zhAn3z"
   },
   "outputs": [],
   "source": [
    "def test(model, word, device = 'cpu'):\n",
    "    model = model.eval().to(device)\n",
    "    outputs = infer(model, word, 30, device)\n",
    "    hindi_output = ''\n",
    "    for out in outputs:\n",
    "        val, indices = out.topk(1)\n",
    "        index = indices.tolist()[0][0]\n",
    "        if index == 0:\n",
    "            break\n",
    "        hindi_char = hindi_alphabets[index+1]\n",
    "        hindi_output += hindi_char\n",
    "    print(word + ' - ' + hindi_output)\n",
    "    return hindi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT8bibYl7CgX"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(model, device = 'cpu'):\n",
    "    model = model.eval().to(device)\n",
    "    predictions = []\n",
    "    accuracy = 0\n",
    "    for i in range(len(test_data)):\n",
    "        eng, hindi = test_data[i]\n",
    "        gt = gt_rep(hindi, hindi_alpha2index, device)\n",
    "        outputs = infer(net, eng, gt.shape[0], device)\n",
    "        correct = 0\n",
    "        for index, out in enumerate(outputs):\n",
    "            val, indices = out.topk(1)\n",
    "            hindi_pos = indices.tolist()[0]\n",
    "            if hindi_pos[0] == gt[index][0]:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy += correct/gt.shape[0]\n",
    "    accuracy /= len(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "dy1bQiORAs5o",
    "outputId": "183c661a-56c9-4f31-a894-188207c58104"
   },
   "outputs": [],
   "source": [
    "accuracy = calc_accuracy(net) * 100\n",
    "accuracy_attn = calc_accuracy(net_att) * 100\n",
    "print('Accuracy w/o attention ', accuracy)\n",
    "print('Acurracy with attention', accuracy_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification - one among 129. \n",
    "\n",
    "(1/129) * 100 is baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukoNAs8wP-GH"
   },
   "source": [
    "no batching above - first packing... then use batching.\n",
    "\n",
    "\n",
    "visualise attention - visualise the alpha values - for what part of input is it giving more focus at a step.. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
